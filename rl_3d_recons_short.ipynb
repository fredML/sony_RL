{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "path = os.getcwd()\n",
    "os.chdir('/mnt/diskSustainability/frederic/sony_RL/sony_RL/base_functions')\n",
    "from dm_env_sphere import SphereEnv\n",
    "os.chdir(path)\n",
    "\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import haiku as hk\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "##\n",
    "\n",
    "list_holes = []\n",
    "objects_path = []\n",
    "object_name = []\n",
    "\n",
    "for k in range (10):\n",
    "    if k==3:\n",
    "        continue\n",
    "    path_k = f'/mnt/diskSustainability/frederic/scanner-gym_models_v2/random_spheres/random_sphere_{k}/'\n",
    "    objects_path.append(path_k)\n",
    "    object_name.append(f'random_sphere_{k}.obj')\n",
    "    list_holes.append(json.load(open(path_k + f'random_sphere_{k}.json')))\n",
    "\n",
    "env = SphereEnv(\n",
    "                objects_path, object_name, img_shape=128, list_holes=list_holes, rmax_T=0.9, max_T=50, theta_n_positions=8, \n",
    "                continuous=True\n",
    "                )\n",
    "env_test = SphereEnv(\n",
    "                    objects_path, object_name, img_shape=128, list_holes=list_holes, rmax_T=0.9, max_T=50, theta_n_positions=8, \n",
    "                    continuous=True,\n",
    "                     )\n",
    "ts = env.reset()\n",
    "\n",
    "##\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '0.8'\n",
    "\n",
    "os.chdir('/mnt/diskSustainability/frederic/sony_RL/sony_RL/rl') \n",
    "import vae\n",
    "from sac import SAC\n",
    "from sac_ae import SAC_AE\n",
    "from dqn import DQN\n",
    "from base_trainer import Trainer\n",
    "os.chdir(path)\n",
    "\n",
    "input_size = 128\n",
    "filter_sizes = [16,32,64,128]\n",
    "input_channels = output_channels = 1\n",
    "final_activation = jax.jit(lambda s:s)\n",
    "\n",
    "latent_dim = 14\n",
    "lambda_kl = 9.21e-5\n",
    "\n",
    "def classic_vae(s, is_training):\n",
    "    return vae.VAE(input_size, latent_dim, filter_sizes, output_channels, final_activation, coord_conv=True)(s, is_training)\n",
    "\n",
    "print('##### VAE initialization #####')\n",
    "\n",
    "vae_init, vae_apply  = hk.without_apply_rng(hk.transform_with_state(classic_vae))\n",
    "vae_apply_jit = jax.jit(vae_apply, static_argnums=3)\n",
    "\n",
    "weights = jnp.load('/mnt/diskSustainability/frederic/sony_RL/params_vae_lat=14_kl=9.21e-05.npz', allow_pickle=True)\n",
    "params_vae = weights['params_vae'][()]\n",
    "bn_vae_state =  weights['bn_vae_state'][()]\n",
    "\n",
    "print()\n",
    "print('##### Initialization finished #####')\n",
    "\n",
    "##\n",
    "\n",
    "seed = np.random.randint(100)\n",
    "print()\n",
    "print('seed = {}'.format(seed))\n",
    "print()\n",
    "print('##### Agent initialization #####')\n",
    "encoder = (vae_apply_jit, params_vae, bn_vae_state)\n",
    "gamma = 0.6\n",
    "scale_reward = 5\n",
    "agent_params = {'gamma':gamma}\n",
    "\n",
    "agent = SAC_AE(num_agent_steps=10**6, state_space=np.empty(env.observation_shape), action_space=np.empty((1,1)), \n",
    "           seed=seed, start_steps=10**3, gamma=gamma, buffer_size=10**3, batch_size=32, encoder=encoder, scale_reward=scale_reward,\n",
    "           beta=lambda_kl)\n",
    "\n",
    "'''agent = DQN(num_agent_steps=10**6, state_space=np.empty(env.observation_shape), action_space=np.array(list(env.actions.keys())), \n",
    "           seed=seed, start_steps=10**3, gamma=gamma, buffer_size=10**3, batch_size=32, encoder=encoder, use_goal=True)\n",
    "'''\n",
    "print()\n",
    "print('##### Initialization finished #####')\n",
    "print()\n",
    "print('##### Training RL agent #####')\n",
    "\n",
    "log_dir = 'sac_joint_vae_10env_logs/'\n",
    "\n",
    "trainer = Trainer(\n",
    "        env=env,\n",
    "        env_test=env_test,\n",
    "        algo=agent,\n",
    "        log_dir=log_dir,\n",
    "        num_agent_steps=10**6,\n",
    "        action_repeat=1,\n",
    "        eval_interval=10**3,\n",
    "        save_params=True,\n",
    "        save_interval=10**4\n",
    "    )\n",
    "\n",
    "with open(os.path.join('/mnt/diskSustainability/frederic/sony_RL/', log_dir, 'hyperparameters.json'), 'w') as f:\n",
    "    json.dump(agent_params, f)\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "#agent.load_params('/mnt/diskSustainability/frederic/sony_RL/'+ log_dir+ '/param/step1000000')\n",
    "print()\n",
    "print('##### Training finished #####')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "ts = env_test.reset(obj=j)\n",
    "print(env_test.current_obj)\n",
    "M = env_test.current_spc.neigh_ijk\n",
    "rewards = []\n",
    "for k in range (10):\n",
    "    action = agent.select_action(ts.observation)\n",
    "    ts = env_test.step(action)\n",
    "    rewards.append(ts.reward)\n",
    "    if ts.step_type == 2:\n",
    "        print('finished in {} steps'.format(k+1))\n",
    "        break\n",
    "print(rewards)\n",
    "print(env_test.total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angles = np.array(env_test.visited_positions)\n",
    "theta = angles[:,0]\n",
    "phi = angles[:,1]\n",
    "\n",
    "if not env_test.continuous:\n",
    "    theta = (theta+1)*np.pi/(2*env_test.theta_n_positions)\n",
    "    phi = phi*2*np.pi/env_test.phi_n_positions\n",
    "\n",
    "R = 5\n",
    "a = R*np.sin(theta)*np.cos(phi)\n",
    "b = R*np.sin(theta)*np.sin(phi)\n",
    "c = R*np.cos(theta)\n",
    "\n",
    "a = (a+4.97)/0.4\n",
    "b = (b+4.97)/0.4\n",
    "c = (c+4.97)/0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for i in range (len(a)):\n",
    "    l.append([i/len(a),'rgb'+str(plt.get_cmap('jet', len(a))(i,bytes=True)[:3])])\n",
    "    l.append([(i+1)/len(a),'rgb'+str(plt.get_cmap('jet', len(a))(i,bytes=True)[:3])])\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter3d(x=a,y=b,z=c,marker=dict(\n",
    "        color=np.arange(len(a)),\n",
    "        colorscale=l,                \n",
    "        colorbar=dict(thickness=20,title={\n",
    "        'text': 'Timesteps','side':'bottom'},\n",
    "           tick0=0,dtick=1,x=0.8, y=0.4, len=0.75)),\n",
    "        text=[str(k) for k in range(len(a))],hoverinfo='text',showlegend=False))\n",
    "fig.add_trace(go.Scatter3d(x=M[:,0],y=M[:,1],z=M[:,2],mode='markers',showlegend=False))\n",
    "fig.update_layout(margin=dict(l=0, r=0, b=0, t=0),hovermode='closest', width=700, height=450,title={\n",
    "        'text': f'Trajectory using SAC(env={j})',\n",
    "        'y':0.9,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'}\n",
    "           )\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
